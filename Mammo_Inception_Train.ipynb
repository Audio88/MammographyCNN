{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import models, layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.applications import InceptionV3\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Breast Cancer with Inception V3 and Transfer Learning\n",
    "\n",
    "Inception V3 is a great convolutional neural network created by google. This architecture uses a lot of parameter reduction techniques to keep it's parameters very low. Only 21 million parameters , but there are 94 convolutional layers in this network. It's a very deep network. \n",
    "\n",
    "Transfer learning is a powerful technique, because it saves us a lot of training time by transfering pre-trained weights. The weights we used were trained on the popular machine learning competition data set imagenet. InceptionV3 was the top performing model in 2015 under 25mil parameters.\n",
    "\n",
    "There are many different approaches to transfer learning. You can freeze the first couple of layers, or you can freeze all but the last layers, and only train on a few of the layers. In both these techniques you have to modify the output of the model to fit your new problem. Another transfer learning technique is to transfer some of the weights from a few of the layers to a completely new model. \n",
    "\n",
    "The approach we used in this data set is to train over the whole model with a very small learning rate. This is a good technique for this problem because mammograms are very different from the images you see in the imagenet challenge. However, the pre-trained weights and bias are a lot stronger than random weights and that saves us a lot of time, often these networks with out pretrained weights will takes days to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loops through ImageDataGenerator and grabs true labels and predictions\n",
    "#and reports metrics with classification_report method\n",
    "def predict_and_report(gen, model):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    gen.reset()\n",
    "    for img, label in gen:\n",
    "        #get true labels for batch and store them\n",
    "        y_true.extend([int(z[1]) for z in label])\n",
    "        #Get predictions as probabilities\n",
    "        batch_pred = model.predict_on_batch(img)\n",
    "        #turn probabilities to class labels and store\n",
    "        batch_pred = np.argmax(batch_pred, axis=1)\n",
    "        y_pred.extend(batch_pred)\n",
    "        #break loop\n",
    "        if gen.batch_index == 0:\n",
    "            break\n",
    "            \n",
    "    print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "    print('Area Under the Receiver Operating Characteristic Curve:', roc_auc_score(y_true, y_pred)) #Area under the curve\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "As for the network architecture itself we are going to leave that up to the smart people at google who created the inception V3 network. However, this still leaves us with quiet a few parameters to adjust ourselves. Here are the parameters: Batch size, learning rate, decay, number of epochs, and data augmentation.\n",
    "\n",
    "For batch size we chose the highest batch size that our GPU memory would allow. This is a batch size of 64, this is a strong batch size even if we had more computational power. A higher batch size means we have a better representation of the data set. Which means our model is going to converge faster and have less variation when the model trys to find the optimal minimum in our gradient. \n",
    "\n",
    "For learning rate I tested different learning rates for one epoch. I chose the learning rate that was one decimal place lower than the learning rate that gave us the lowest validation loss after one epoch. This learning rate was 1e-5. If our learning rate is too high then the pre-trained weights will be destroyed and our model will have very poor results. If the learning rate is very low the model might not learn fast enough. So we went with a very safe learning rate and trained for a bit longer then we needed. This model took us about 10 hours to train on a local GPU.\n",
    "\n",
    "For decay we just did a linear decay. Which was learning rate divided by the number of epochs, and subtracted that number after every epoch. Using this technique the learning rate is near zero by the last epoch. By lowering the learning rate after each epoch we can be sure that our model has a better chance of reaching an optimal minimum in our gradient.\n",
    "\n",
    "As for data augmentation we went with a pretty standard set of augmenation. There's a shear, a zoom, and both horizontal and vertical flips. I assumed that cancer has no orientation. these augmentations produced good results.\n",
    "\n",
    "One important technique we used was a model check point. This saves the model weights for each epoch that has the best score. We can use these weights that were saved at a mid point during training. This helps us ensure that the model weights we use do not over fit on the training data, and generalize well to new data. It is also important just to keep the model weights with the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39011 images belonging to 2 classes.\n",
      "Found 8436 images belonging to 2 classes.\n",
      "Found 8438 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'C:\\\\users\\\\will\\\\ds\\\\mammo\\\\train'\n",
    "valid_dir = 'C:\\\\users\\\\will\\\\ds\\\\mammo\\\\validation'\n",
    "test_dir = 'C:\\\\users\\\\will\\\\ds\\\\mammo\\\\test'\n",
    "\n",
    "img_width, img_height = 299, 299\n",
    "batch_size = 64\n",
    "num_epochs= 75\n",
    "learning_rate = 1e-5\n",
    "#First epoch loss for different learning rates.\n",
    "#1e-3 loss: 0.0679 - acc: 0.8581 - val_loss: 1.7522 - val_acc: 0.5769\n",
    "#1e-4 loss: 0.0449 - acc: 0.9106 - val_loss: 0.1305 - val_acc: 0.9489\n",
    "#1e-5 loss: 0.0702 - acc: 0.8506 - val_loss: 0.1667 - val_acc: 0.9398\n",
    "decay = learning_rate / num_epochs\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb') #using RGB here because inception V3 is a 3 channel model\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    valid_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    color_mode='rgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 149, 149, 32) 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 32) 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 147, 147, 64) 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 73, 73, 80)   5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73, 73, 80)   240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 73, 73, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 71, 71, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 71, 71, 192)  576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 71, 71, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 35, 35, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 35, 35, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 35, 35, 48)   9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 35, 35, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 35, 35, 48)   144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 35, 35, 96)   288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 35, 35, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 35, 35, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 192)  0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 35, 35, 64)   12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 35, 35, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 35, 35, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 35, 35, 64)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 96)   288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 32)   96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 35, 35, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 35, 35, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 35, 35, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 48)   144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 64)   192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 96)   288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 35, 35, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 48)   144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 35, 35, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 35, 35, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 64)   192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 96)   288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 35, 35, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 35, 35, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 35, 35, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 64)   192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 35, 35, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 35, 35, 96)   288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 35, 35, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 17, 17, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 17, 17, 96)   82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 17, 17, 384)  1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 17, 17, 96)   288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 17, 17, 384)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 17, 17, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 17, 17, 768)  0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 17, 17, 128)  384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 17, 17, 128)  0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 17, 17, 128)  114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 17, 17, 128)  384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 17, 17, 128)  0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 17, 17, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 17, 17, 128)  114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 17, 17, 128)  384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 17, 17, 128)  384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 17, 17, 128)  0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 17, 17, 128)  0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 17, 17, 128)  114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 17, 17, 128)  114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 17, 17, 128)  384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 17, 17, 128)  384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 17, 17, 128)  0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 17, 17, 128)  0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 17, 17, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 17, 17, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 17, 17, 192)  172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 17, 17, 192)  172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 17, 17, 192)  576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 17, 17, 192)  576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 17, 17, 192)  576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 192)  576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 17, 17, 192)  0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 17, 17, 192)  0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 17, 17, 192)  0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 192)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 17, 17, 768)  0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 160)  480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 160)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 160)  179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 160)  480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 160)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 17, 17, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 160)  179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 17, 17, 160)  480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 160)  480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 17, 17, 160)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 160)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 160)  179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 160)  179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 160)  480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 160)  480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 160)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 160)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 17, 17, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 192)  215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 17, 17, 192)  576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 192)  576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 192)  576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 17, 17, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 192)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 192)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 17, 17, 768)  0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 160)  480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 160)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 160)  179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 160)  480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 160)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 160)  179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 160)  480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 160)  480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 160)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 160)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 160)  179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 160)  179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 160)  480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 160)  480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 160)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 160)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 192)  215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 192)  576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 192)  576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 192)  576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 192)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 192)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 192)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 17, 17, 768)  0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 192)  258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 192)  576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 192)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 192)  258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 192)  576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 192)  576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 192)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 192)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 192)  258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 192)  576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 192)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 17, 17, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 192)  258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 192)  147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 192)  576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 192)  576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 192)  576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 192)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 192)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 192)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 17, 17, 768)  0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 192)  576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 192)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 192)  258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 192)  576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 192)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 17, 17, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 17, 17, 192)  258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 17, 17, 192)  576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 17, 17, 192)  576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 17, 17, 192)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 8, 8, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 320)    960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 8, 8, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 8, 8, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 8, 8, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 8, 8, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 8, 8, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 8, 8, 448)    1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 8, 8, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 8, 8, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 8, 8, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 8, 8, 384)    1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 8, 8, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 8, 8, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 8, 8, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 8, 8, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 8, 8, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 8, 8, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 8, 8, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 8, 8, 384)    1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 8, 8, 384)    1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 8, 8, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 8, 8, 384)    1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 8, 8, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 8, 8, 320)    960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 8, 8, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 8, 8, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 8, 8, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 8, 8, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 8, 8, 192)    576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 8, 8, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 8, 8, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 8, 8, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 8, 8, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 448)    1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 8, 8, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 8, 8, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 8, 8, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 8, 8, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 8, 8, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 8, 8, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 8, 8, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 384)    1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 384)    1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 384)    1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 8, 8, 320)    960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 8, 8, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 8, 8, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 8, 8, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 8, 8, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 192)    576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 8, 8, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 8, 8, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 8, 8, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 8, 8, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 2)            4098        avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 21,806,882\n",
      "Trainable params: 21,772,450\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Load the InceptionV3 model and imagenet weights\n",
    "IV3 = InceptionV3(weights='imagenet', include_top=True, input_shape=(img_width, img_height, 3))    \n",
    "\n",
    "# modify output for 2 classes\n",
    "fc2 = IV3.get_layer('avg_pool').output\n",
    "predictions = Dense(2, activation='softmax', name='predictions')(fc2)\n",
    "model = Model(inputs=IV3.input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "610/610 [==============================] - 529s 866ms/step - loss: 0.0691 - acc: 0.8560 - val_loss: 0.1940 - val_acc: 0.9266\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19396, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-01-0.93.hdf5\n",
      "Epoch 2/75\n",
      "610/610 [==============================] - 499s 819ms/step - loss: 0.0391 - acc: 0.9286 - val_loss: 0.1456 - val_acc: 0.9484\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19396 to 0.14565, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-02-0.95.hdf5\n",
      "Epoch 3/75\n",
      "610/610 [==============================] - 500s 819ms/step - loss: 0.0325 - acc: 0.9433 - val_loss: 0.1268 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14565 to 0.12681, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-03-0.95.hdf5\n",
      "Epoch 4/75\n",
      "610/610 [==============================] - 499s 817ms/step - loss: 0.0269 - acc: 0.9510 - val_loss: 0.1526 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.12681\n",
      "Epoch 5/75\n",
      "610/610 [==============================] - 499s 818ms/step - loss: 0.0247 - acc: 0.9545 - val_loss: 0.0999 - val_acc: 0.9619\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.12681 to 0.09988, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-05-0.96.hdf5\n",
      "Epoch 6/75\n",
      "610/610 [==============================] - 499s 818ms/step - loss: 0.0219 - acc: 0.9601 - val_loss: 0.0979 - val_acc: 0.9618\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09988 to 0.09793, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-06-0.96.hdf5\n",
      "Epoch 7/75\n",
      "610/610 [==============================] - 499s 818ms/step - loss: 0.0201 - acc: 0.9646 - val_loss: 0.0811 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09793 to 0.08108, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-07-0.97.hdf5\n",
      "Epoch 8/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0182 - acc: 0.9664 - val_loss: 0.0837 - val_acc: 0.9685\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.08108\n",
      "Epoch 9/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0161 - acc: 0.9701 - val_loss: 0.0752 - val_acc: 0.9711\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08108 to 0.07515, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-09-0.97.hdf5\n",
      "Epoch 10/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0154 - acc: 0.9717 - val_loss: 0.0896 - val_acc: 0.9638\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.07515\n",
      "Epoch 11/75\n",
      "610/610 [==============================] - 499s 818ms/step - loss: 0.0137 - acc: 0.9746 - val_loss: 0.0815 - val_acc: 0.9702\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.07515\n",
      "Epoch 12/75\n",
      "610/610 [==============================] - 532s 873ms/step - loss: 0.0139 - acc: 0.9741 - val_loss: 0.0656 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.07515 to 0.06556, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-12-0.98.hdf5\n",
      "Epoch 13/75\n",
      "610/610 [==============================] - 523s 858ms/step - loss: 0.0122 - acc: 0.9774 - val_loss: 0.0669 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06556\n",
      "Epoch 14/75\n",
      "610/610 [==============================] - 523s 857ms/step - loss: 0.0115 - acc: 0.9786 - val_loss: 0.0676 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06556\n",
      "Epoch 15/75\n",
      "610/610 [==============================] - 513s 841ms/step - loss: 0.0100 - acc: 0.9816 - val_loss: 0.0659 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.06556\n",
      "Epoch 16/75\n",
      "610/610 [==============================] - 521s 854ms/step - loss: 0.0095 - acc: 0.9822 - val_loss: 0.0757 - val_acc: 0.9737\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06556\n",
      "Epoch 17/75\n",
      "610/610 [==============================] - 519s 851ms/step - loss: 0.0096 - acc: 0.9828 - val_loss: 0.0697 - val_acc: 0.9749\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06556\n",
      "Epoch 18/75\n",
      "610/610 [==============================] - 516s 846ms/step - loss: 0.0085 - acc: 0.9842 - val_loss: 0.0577 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06556 to 0.05774, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-18-0.98.hdf5\n",
      "Epoch 19/75\n",
      "610/610 [==============================] - 519s 850ms/step - loss: 0.0083 - acc: 0.9850 - val_loss: 0.0671 - val_acc: 0.9761\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.05774\n",
      "Epoch 20/75\n",
      "610/610 [==============================] - 510s 837ms/step - loss: 0.0074 - acc: 0.9860 - val_loss: 0.0689 - val_acc: 0.9769\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.05774\n",
      "Epoch 21/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0076 - acc: 0.9860 - val_loss: 0.0915 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.05774\n",
      "Epoch 22/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0063 - acc: 0.9886 - val_loss: 0.0757 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.05774\n",
      "Epoch 23/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0062 - acc: 0.9881 - val_loss: 0.0646 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.05774\n",
      "Epoch 24/75\n",
      "610/610 [==============================] - 497s 816ms/step - loss: 0.0060 - acc: 0.9891 - val_loss: 0.0574 - val_acc: 0.9819\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05774 to 0.05744, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-24-0.98.hdf5\n",
      "Epoch 25/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0051 - acc: 0.9903 - val_loss: 0.0857 - val_acc: 0.9721\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.05744\n",
      "Epoch 26/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0058 - acc: 0.9891 - val_loss: 0.0588 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.05744\n",
      "Epoch 27/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0056 - acc: 0.9898 - val_loss: 0.0567 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.05744 to 0.05666, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-27-0.98.hdf5\n",
      "Epoch 28/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0049 - acc: 0.9910 - val_loss: 0.0619 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.05666\n",
      "Epoch 29/75\n",
      "610/610 [==============================] - 500s 819ms/step - loss: 0.0049 - acc: 0.9907 - val_loss: 0.0600 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.05666\n",
      "Epoch 30/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0041 - acc: 0.9928 - val_loss: 0.0637 - val_acc: 0.9804\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.05666\n",
      "Epoch 31/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0037 - acc: 0.9933 - val_loss: 0.0592 - val_acc: 0.9807\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.05666\n",
      "Epoch 32/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0042 - acc: 0.9922 - val_loss: 0.0696 - val_acc: 0.9806\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.05666\n",
      "Epoch 33/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0044 - acc: 0.9921 - val_loss: 0.0530 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.05666 to 0.05301, saving model to C:\\users\\will\\ds\\mammo-weights-transfer\\weights-improvement-33-0.98.hdf5\n",
      "Epoch 34/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0042 - acc: 0.9927 - val_loss: 0.0620 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.05301\n",
      "Epoch 35/75\n",
      "610/610 [==============================] - 499s 818ms/step - loss: 0.0041 - acc: 0.9928 - val_loss: 0.0607 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.05301\n",
      "Epoch 36/75\n",
      "610/610 [==============================] - 503s 825ms/step - loss: 0.0031 - acc: 0.9943 - val_loss: 0.0585 - val_acc: 0.9846\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.05301\n",
      "Epoch 37/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0034 - acc: 0.9937 - val_loss: 0.0596 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.05301\n",
      "Epoch 38/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0032 - acc: 0.9940 - val_loss: 0.0683 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.05301\n",
      "Epoch 39/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0032 - acc: 0.9944 - val_loss: 0.0710 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.05301\n",
      "Epoch 40/75\n",
      "610/610 [==============================] - 496s 813ms/step - loss: 0.0030 - acc: 0.9948 - val_loss: 0.0579 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.05301\n",
      "Epoch 41/75\n",
      "610/610 [==============================] - 497s 814ms/step - loss: 0.0026 - acc: 0.9949 - val_loss: 0.0659 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.05301\n",
      "Epoch 42/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0033 - acc: 0.9943 - val_loss: 0.0668 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.05301\n",
      "Epoch 43/75\n",
      "610/610 [==============================] - 497s 814ms/step - loss: 0.0024 - acc: 0.9955 - val_loss: 0.0636 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.05301\n",
      "Epoch 44/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0029 - acc: 0.9951 - val_loss: 0.0619 - val_acc: 0.9841\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.05301\n",
      "Epoch 45/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0031 - acc: 0.9950 - val_loss: 0.0653 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.05301\n",
      "Epoch 46/75\n",
      "610/610 [==============================] - 498s 817ms/step - loss: 0.0025 - acc: 0.9956 - val_loss: 0.0666 - val_acc: 0.9838\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.05301\n",
      "Epoch 47/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0026 - acc: 0.9955 - val_loss: 0.0764 - val_acc: 0.9803\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.05301\n",
      "Epoch 48/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0025 - acc: 0.9956 - val_loss: 0.0654 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.05301\n",
      "Epoch 49/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0019 - acc: 0.9967 - val_loss: 0.0633 - val_acc: 0.9835\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.05301\n",
      "Epoch 50/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0029 - acc: 0.9948 - val_loss: 0.0704 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.05301\n",
      "Epoch 51/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0023 - acc: 0.9959 - val_loss: 0.0662 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.05301\n",
      "Epoch 52/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0020 - acc: 0.9968 - val_loss: 0.0670 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.05301\n",
      "Epoch 53/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0021 - acc: 0.9962 - val_loss: 0.0733 - val_acc: 0.9842\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.05301\n",
      "Epoch 54/75\n",
      "610/610 [==============================] - 497s 816ms/step - loss: 0.0021 - acc: 0.9963 - val_loss: 0.0668 - val_acc: 0.9844\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.05301\n",
      "Epoch 55/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0019 - acc: 0.9968 - val_loss: 0.0658 - val_acc: 0.9862\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.05301\n",
      "Epoch 56/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0017 - acc: 0.9972 - val_loss: 0.0632 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.05301\n",
      "Epoch 57/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0022 - acc: 0.9965 - val_loss: 0.0713 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.05301\n",
      "Epoch 58/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0020 - acc: 0.9964 - val_loss: 0.0658 - val_acc: 0.9865\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.05301\n",
      "Epoch 59/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0023 - acc: 0.9964 - val_loss: 0.0662 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.05301\n",
      "Epoch 60/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0018 - acc: 0.9967 - val_loss: 0.0653 - val_acc: 0.9868\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.05301\n",
      "Epoch 61/75\n",
      "610/610 [==============================] - 497s 814ms/step - loss: 0.0017 - acc: 0.9972 - val_loss: 0.0779 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.05301\n",
      "Epoch 62/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0020 - acc: 0.9967 - val_loss: 0.0612 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.05301\n",
      "Epoch 63/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0016 - acc: 0.9969 - val_loss: 0.0617 - val_acc: 0.9857\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.05301\n",
      "Epoch 64/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0019 - acc: 0.9966 - val_loss: 0.0617 - val_acc: 0.9848\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.05301\n",
      "Epoch 65/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0019 - acc: 0.9965 - val_loss: 0.0630 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.05301\n",
      "Epoch 66/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0015 - acc: 0.9976 - val_loss: 0.0638 - val_acc: 0.9854\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.05301\n",
      "Epoch 67/75\n",
      "610/610 [==============================] - 497s 814ms/step - loss: 0.0020 - acc: 0.9966 - val_loss: 0.0639 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.05301\n",
      "Epoch 68/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0017 - acc: 0.9972 - val_loss: 0.0672 - val_acc: 0.9870\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.05301\n",
      "Epoch 69/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0012 - acc: 0.9980 - val_loss: 0.0678 - val_acc: 0.9866\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05301\n",
      "Epoch 70/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0015 - acc: 0.9973 - val_loss: 0.0840 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05301\n",
      "Epoch 71/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0020 - acc: 0.9967 - val_loss: 0.0756 - val_acc: 0.9853\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05301\n",
      "Epoch 72/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0018 - acc: 0.9974 - val_loss: 0.0726 - val_acc: 0.9839\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.05301\n",
      "Epoch 73/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0016 - acc: 0.9972 - val_loss: 0.0609 - val_acc: 0.9860\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.05301\n",
      "Epoch 74/75\n",
      "610/610 [==============================] - 498s 816ms/step - loss: 0.0012 - acc: 0.9982 - val_loss: 0.0674 - val_acc: 0.9858\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.05301\n",
      "Epoch 75/75\n",
      "610/610 [==============================] - 497s 815ms/step - loss: 0.0016 - acc: 0.9973 - val_loss: 0.0662 - val_acc: 0.9855\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.05301\n"
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=learning_rate, decay=decay)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Save best weights\n",
    "filepath=\"C:\\\\users\\\\will\\\\ds\\\\mammo-weights-transfer\\\\weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "result = model.fit_generator(\n",
    "            train_generator,\n",
    "            epochs=num_epochs,\n",
    "            verbose = 1,\n",
    "            class_weight= {0:.13, 1:.87},\n",
    "            validation_data = validation_generator,\n",
    "            callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(result.history['loss'],'r')\n",
    "plt.plot(result.history['val_loss'],'g')\n",
    "plt.xticks(np.arange(0, num_epochs+1, 10.0))\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "This is a healthy learning rate, the epoch with the best score was epoch 33. We used these weights saved at epoch 33 below to see how accurate the model is on validation and train data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9848269321953532\n",
      "Area Under the Receiver Operating Characteristic Curve: 0.9648074315413139\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99      7343\n",
      "          1       0.94      0.94      0.94      1093\n",
      "\n",
      "avg / total       0.98      0.98      0.98      8436\n",
      "\n",
      "Accuracy: 0.9974366204403886\n",
      "Area Under the Receiver Operating Characteristic Curve: 0.9971101228009448\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     33909\n",
      "          1       0.98      1.00      0.99      5102\n",
      "\n",
      "avg / total       1.00      1.00      1.00     39011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('mammo-weights-transfer\\weights-improvement-33-0.98.hdf5')\n",
    "predict_and_report(validation_generator, model)\n",
    "predict_and_report(train_generator, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
